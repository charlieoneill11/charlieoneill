---
title: "A review of AGI trajectories"
description: "Comparing symbolic and empirical approaches"
layout: post
toc: true
comments: true
hide: false
search_exclude: true
categories: [AGI, deep learning, linguistics]
---

## A path towards autonomous machine intelligence

In its simplest terms, the idea of JEPA is that it is **not** generative.

## Comparing JEPA and closed loop feedback (LeCun vs Ma)

## Large language models (LLMs)
The main reason LeCun believes scaling will not be enough to lead to emergence is that LLMs are, by construction, generative. Inputs are converted into a sequence of discrete tokens, which works well for text, but flounder in the face of high-dimensional and continuous spaces.

And yet LeCun remains ever the empiricist. "I speculate that common sense may emerge from learning world models that capture the self-consistency and mutual dependencies of observations in the world, allowing an agent to fill in missing information and detect violations of its world model." 

## What needs to be done?
According to LeCun, there are several non-trivial problems which constitute the "known unknowns" of his JEPA-based architecture. Probably 